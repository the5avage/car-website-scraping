{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc55e48",
   "metadata": {},
   "source": [
    "### Test: BERT large uncased\n",
    "\n",
    "Model reference and usage description: https://huggingface.co/google-bert/bert-large-uncased\n",
    "\n",
    "Example usage: Feeding an input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd9346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381bad1",
   "metadata": {},
   "source": [
    "BERT does transform the input text / input tokens into the vector space.  \n",
    "The `last_hidden_state` contains the final embeddings for each token in the sequence.\n",
    "\n",
    "The pooler applies a fully connected layer to the `[CLS]` token to obtain a condensed representation of the entire sequence.  \n",
    "This condensed representation can be used in a classification task, as this single vector now represents the whole input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b645aae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Hidden State Shape: torch.Size([1, 12, 1024])\n",
      "Pooler Output Shape: torch.Size([1, 1024])\n",
      "Tokens: ['[CLS]', 'replace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.', '[SEP]']\n",
      "Embedding of first token (CLS): tensor([-0.1534, -0.9412, -0.6168,  ..., -0.7690, -0.0030,  0.2449],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Embedding of the first word token: tensor([-0.5923, -0.7163, -0.9268,  ...,  0.4954,  0.4566,  0.0285],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Pooler Output (CLS after pooling): tensor([[-0.9995, -0.9970,  1.0000,  ..., -1.0000,  0.9944, -0.9978]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "last_hidden_state = output.last_hidden_state\n",
    "pooler_output = output.pooler_output\n",
    "\n",
    "# Print the shapes of the outputs\n",
    "print(\"Last Hidden State Shape:\", last_hidden_state.shape) # Shape: [batch_size, sequence_length, hidden_size]\n",
    "print(\"Pooler Output Shape:\", pooler_output.shape) # Shape: [batch_size, hidden_size]\n",
    "\n",
    "# Decode the tokens back to see how BERT splits the input\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Embedding of first token (CLS): {last_hidden_state[0][0]}\")  # CLS token embedding\n",
    "print(f\"Embedding of the first word token: {last_hidden_state[0][1]}\")  # First word's token embedding\n",
    "\n",
    "# Embedding of the CLS token after pooling\n",
    "print(f\"Pooler Output (CLS after pooling): {pooler_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e41cd",
   "metadata": {},
   "source": [
    "Explanation:  \n",
    "batch_size: Number of model inputs. If multiple sentences are processed in parallel, the batch size will be greater than 1.  \n",
    "sequence_length: number of tokens in a given input sentence or text. BERT has a maximum sequence length of 512 tokens !!!  \n",
    "hidden_size: fixed number of neurons in each layer. `bert-base-uncased` has 768, `bert-large-uncased` has 1024 neurons per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55096b94",
   "metadata": {},
   "source": [
    "### Test: Sentence-BERT (SBERT)\n",
    "This version of BERT is optimized for comparing sentence similarity or finding related pairs.\n",
    "\n",
    "Used as bi-encoder: encodes both texts separately → compares embeddings via cosine similarity.\n",
    "\n",
    "requirement: `pip3 install -U sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046effa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.7127\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Texts\n",
    "short_text = (\n",
    "    \"I'm looking for a white diesel van with automatic transmission, registered recently, \"\n",
    "    \"less than 30,000 km driven, and EURO 6 emission class.\"\n",
    ")\n",
    "\n",
    "long_text = \"\"\"\n",
    "Information:\n",
    "Category: Van / minibus, 5 door. Engine type: Diesel. Fuel type: Diesel. Emission class: EURO 6. \n",
    "CO2 emissions: 173 g/km (combined). Power output: 110 KW / 150 PS. First registration: 12.2023. \n",
    "KBA Key Manufacturer: 0603. KBA Key Type: CQJ. VIN: WV2ZZZST2RH******. Transmission: Automatic. \n",
    "Colour: white (Sonderlackierung Candy-weiss). Read mileage: 21,800 Kilometres. \n",
    "Owners: 1. Location: D-73. Vehicle release: 3 working days after payment.\n",
    "\"\"\"\n",
    "\n",
    "# Embeddings\n",
    "short_emb = model.encode(short_text, convert_to_tensor=True)\n",
    "long_emb = model.encode(long_text, convert_to_tensor=True)\n",
    "\n",
    "# Cosine similarity\n",
    "score = util.cos_sim(short_emb, long_emb).item()\n",
    "print(f\"Similarity score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345dd254",
   "metadata": {},
   "source": [
    "Test: Checking the cosine similarity for different car texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3585b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score (short_text_1): 0.7127\n",
      "Similarity score (short_text_2): 0.6525\n",
      "Similarity score (short_text_3): 0.3093\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Text\n",
    "long_text = \"\"\"\n",
    "Information:\n",
    "Category: Van / minibus, 5 door. Engine type: Diesel. Fuel type: Diesel. Emission class: EURO 6. \n",
    "CO2 emissions: 173 g/km (combined). Power output: 110 KW / 150 PS. First registration: 12.2023. \n",
    "KBA Key Manufacturer: 0603. KBA Key Type: CQJ. VIN: WV2ZZZST2RH******. Transmission: Automatic. \n",
    "Colour: white (Sonderlackierung Candy-weiss). Read mileage: 21,800 Kilometres. \n",
    "Owners: 1. Location: D-73. Vehicle release: 3 working days after payment.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Matching query from before\n",
    "short_text_1 = (\n",
    "    \"I'm looking for a white diesel van with automatic transmission, registered recently, \"\n",
    "    \"less than 30,000 km driven, and EURO 6 emission class.\"\n",
    ")\n",
    "\n",
    "# 2. Close wording, but doesn't match key facts (manual, petrol, older, wrong emissions)\n",
    "short_text_2 = (\n",
    "    \"I'm searching for a white petrol van with manual transmission, EURO 5 emissions, \"\n",
    "    \"and around 80,000 kilometers mileage.\"\n",
    ")\n",
    "\n",
    "# 3. Completely different car (sports car, different body, color, purpose, etc.)\n",
    "short_text_3 = (\n",
    "    \"I'm interested in a red convertible sports car with over 300 horsepower and leather seats.\"\n",
    ")\n",
    "\n",
    "# Encode all texts\n",
    "long_emb = model.encode(long_text, convert_to_tensor=True)\n",
    "queries = [short_text_1, short_text_2, short_text_3]\n",
    "query_embs = model.encode(queries, convert_to_tensor=True)\n",
    "\n",
    "# Compare each short query to the long description\n",
    "for i, emb in enumerate(query_embs):\n",
    "    sim = util.cos_sim(emb, long_emb).item()\n",
    "    print(f\"Similarity score (short_text_{i+1}): {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177cc8e",
   "metadata": {},
   "source": [
    "### Test: RoBERTa\n",
    "Used as Cross-Encoder (encodes both texts jointly, in a single input pair → outputs scores).\n",
    "\n",
    "This model outputs a probability distribution over 3 classes:  \n",
    "    - entailment → the short sentence is supported by the long one  \n",
    "    - neutral → no strong connection  \n",
    "    - contradiction → the long text contradicts the short text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e204297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short Text 1: I'm looking for a white diesel van with automatic transmission, registered recently, less than 30,000 km driven, and EURO 6 emission class.\n",
      "Entailment Score:     0.8561\n",
      "Neutral Score:        0.0527\n",
      "Contradiction Score:  0.0913\n",
      "\n",
      "Short Text 2: I'm searching for a white petrol van with manual transmission, EURO 5 emissions, and around 80,000 kilometers mileage.\n",
      "Entailment Score:     0.7532\n",
      "Neutral Score:        0.0290\n",
      "Contradiction Score:  0.2178\n",
      "\n",
      "Short Text 3: I'm interested in a red convertible sports car with over 300 horsepower and leather seats.\n",
      "Entailment Score:     0.5132\n",
      "Neutral Score:        0.0251\n",
      "Contradiction Score:  0.4617\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load RoBERTa cross-encoder trained for NLI\n",
    "model = CrossEncoder('cross-encoder/nli-roberta-base')\n",
    "\n",
    "# Long vehicle description\n",
    "long_text = \"\"\"\n",
    "Information:\n",
    "Category: Van / minibus, 5 door. Engine type: Diesel. Fuel type: Diesel. Emission class: EURO 6. \n",
    "CO2 emissions: 173 g/km (combined). Power output: 110 KW / 150 PS. First registration: 12.2023. \n",
    "KBA Key Manufacturer: 0603. KBA Key Type: CQJ. VIN: WV2ZZZST2RH******. Transmission: Automatic. \n",
    "Colour: white (Sonderlackierung Candy-weiss). Read mileage: 21,800 Kilometres. \n",
    "Owners: 1. Location: D-73. Vehicle release: 3 working days after payment.\n",
    "\"\"\"\n",
    "\n",
    "# Short sentences (queries)\n",
    "queries = [\n",
    "    \"I'm looking for a white diesel van with automatic transmission, registered recently, less than 30,000 km driven, and EURO 6 emission class.\",\n",
    "    \"I'm searching for a white petrol van with manual transmission, EURO 5 emissions, and around 80,000 kilometers mileage.\",\n",
    "    \"I'm interested in a red convertible sports car with over 300 horsepower and leather seats.\"\n",
    "]\n",
    "\n",
    "# Predict probabilities: returns [contradiction, neutral, entailment]\n",
    "# This line concatenates each pair into a single input\n",
    "predictions = model.predict([(q, long_text) for q in queries], apply_softmax=True) # [CLS] short_text [SEP] long_text [SEP]\n",
    "\n",
    "# Display results\n",
    "for i, (query, probs) in enumerate(zip(queries, predictions), 1):\n",
    "    print(f\"\\nShort Text {i}: {query}\")\n",
    "    print(f\"Entailment Score:     {probs[2]:.4f}\")\n",
    "    print(f\"Neutral Score:        {probs[1]:.4f}\")\n",
    "    print(f\"Contradiction Score:  {probs[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf84cdc",
   "metadata": {},
   "source": [
    "#### Interpreting the scores\n",
    "\n",
    "| Short Text | Entailment | Neutral | Contradiction | Intended Match? |\n",
    "| ---------- | ---------- | ------- | ------------- | --------------- |\n",
    "| 1          | **0.8561** | 0.0527  | 0.0913        | Yes             |\n",
    "| 2          | **0.7532** | 0.0290  | 0.2178        | No              |\n",
    "| 3          | **0.5132** | 0.0251  | 0.4617        | No              |\n",
    "\n",
    "Short Text 1  \n",
    "    \"I'm looking for a white diesel van with automatic transmission...\"  \n",
    "    Entailment score 0.8561 → Very high, as expected.  \n",
    "    Contradiction only 0.09 → Almost no disagreement.  \n",
    "    Interpretation: The model strongly believes this request is supported by the long vehicle description.  \n",
    "    → Correct behavior \n",
    "\n",
    "Short Text 2  \n",
    "    \"I'm searching for a white petrol van with manual transmission, EURO 5...\"  \n",
    "    Entailment score 0.7532 → Surprisingly high.  \n",
    "    Contradiction 0.2178 → Some disagreement is detected.  \n",
    "    Interpretation: RoBERTa notices mismatches (petrol ≠ diesel, manual ≠ automatic, mileage too high), but still leans toward entailment.  \n",
    "    Why? RoBERTa cross-encoders trained on NLI are not precise with factual constraints, especially numbers or subtle mismatches (like EURO 5 vs EURO 6).  \n",
    "    It weighs semantic similarity heavily.  \n",
    "    → The request \"looks like\" a match at surface level, even if details don’t align.  \n",
    "    → Needs improvement for this task.\n",
    "\n",
    " Short Text 3  \n",
    "    \"I'm interested in a red convertible sports car...\"  \n",
    "    Entailment score 0.5132 → Shockingly high for an obviously mismatching request.  \n",
    "    Contradiction score 0.4617 → High too, indicating model is confused.  \n",
    "    Interpretation: This is a failure case. The model can't confidently reject the query despite large semantic differences.  \n",
    "    → This is a limitation of using RoBERTa (NLI) cross-encoders on very domain-specific matching tasks.  \n",
    "    They aren't trained to penalize numerical, factual, or attribute mismatches strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f015e",
   "metadata": {},
   "source": [
    "### Test: DeBERTa\n",
    "Used as Cross-Encoder (encodes both texts jointly, in a single input pair → outputs scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short Text 1: I'm looking for a white diesel van with automatic transmission, registered recently, less than 30,000 km driven, and EURO 6 emission class.\n",
      "Entailment Score:     0.9986\n",
      "Neutral Score:        0.0007\n",
      "Contradiction Score:  0.0007\n",
      "\n",
      "Short Text 2: I'm searching for a white petrol van with manual transmission, EURO 5 emissions, and around 80,000 kilometers mileage.\n",
      "Entailment Score:     0.0006\n",
      "Neutral Score:        0.0001\n",
      "Contradiction Score:  0.9993\n",
      "\n",
      "Short Text 3: I'm interested in a red convertible sports car with over 300 horsepower and leather seats.\n",
      "Entailment Score:     0.0097\n",
      "Neutral Score:        0.0002\n",
      "Contradiction Score:  0.9902\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load DeBERTa-based NLI cross-encoder\n",
    "model = CrossEncoder('cross-encoder/nli-deberta-v3-base')\n",
    "\n",
    "# Long vehicle description\n",
    "long_text = \"\"\"\n",
    "Information:\n",
    "Category: Van / minibus, 5 door. Engine type: Diesel. Fuel type: Diesel. Emission class: EURO 6. \n",
    "CO2 emissions: 173 g/km (combined). Power output: 110 KW / 150 PS. First registration: 12.2023. \n",
    "KBA Key Manufacturer: 0603. KBA Key Type: CQJ. VIN: WV2ZZZST2RH******. Transmission: Automatic. \n",
    "Colour: white (Sonderlackierung Candy-weiss). Read mileage: 21,800 Kilometres. \n",
    "Owners: 1. Location: D-73. Vehicle release: 3 working days after payment.\n",
    "\"\"\"\n",
    "\n",
    "# Short search texts\n",
    "queries = [\n",
    "    \"I'm looking for a white diesel van with automatic transmission, registered recently, less than 30,000 km driven, and EURO 6 emission class.\",\n",
    "    \"I'm searching for a white petrol van with manual transmission, EURO 5 emissions, and around 80,000 kilometers mileage.\",\n",
    "    \"I'm interested in a red convertible sports car with over 300 horsepower and leather seats.\"\n",
    "]\n",
    "\n",
    "# Predict probabilities: returns [contradiction, neutral, entailment]\n",
    "# This line concatenates each pair into a single input\n",
    "predictions = model.predict([(q, long_text) for q in queries], apply_softmax=True) # [CLS] short_text [SEP] long_text [SEP]\n",
    "\n",
    "# Display results\n",
    "for i, (query, probs) in enumerate(zip(queries, predictions), 1):\n",
    "    print(f\"\\nShort Text {i}: {query}\")\n",
    "    print(f\"Entailment Score:     {probs[2]:.4f}\")\n",
    "    print(f\"Neutral Score:        {probs[1]:.4f}\")\n",
    "    print(f\"Contradiction Score:  {probs[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fba1f",
   "metadata": {},
   "source": [
    "This model performs exceptionally well by only giving the first text an especially high entailment score while seeing the other two as contradictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
