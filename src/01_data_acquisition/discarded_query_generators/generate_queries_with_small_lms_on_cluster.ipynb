{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c98159",
   "metadata": {},
   "source": [
    "### Test: Smaller Language Models that are able to run on a V100\n",
    "\n",
    "In an attempt to generate better serch queries that a theoretical car dealer would use to find a vehicle, we started to run the biggest-possible language models on a V100 in order to see what they might be able to do for us in terms of query-generation.\n",
    "\n",
    "1: Testing GPT-j-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388b20e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 619/619 [00:00<00:00, 4.74MB/s]\n",
      "Downloading vocab.json: 798kB [00:00, 118MB/s]\n",
      "Downloading merges.txt: 456kB [00:00, 131MB/s]\n",
      "Downloading tokenizer.json: 1.37MB [00:00, 141MB/s]\n",
      "Downloading added_tokens.json: 4.04kB [00:00, 24.1MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 357/357 [00:00<00:00, 3.43MB/s]\n",
      "Downloading config.json: 100%|██████████| 930/930 [00:00<00:00, 8.28MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /storage/venv/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"/root/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/out/nls.messages.json\",\"locale\"'), PosixPath('\"en\",\"defaultMessagesFile\"'), PosixPath('{}}'), PosixPath('\"en\",\"osLocale\"'), PosixPath('\"en\",\"resolvedLanguage\"'), PosixPath('{\"userLocale\"'), PosixPath('\"en\",\"availableLanguages\"')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 24.2G/24.2G [01:45<00:00, 229MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is David and I am a recovering alcoholic. I am also a recovering addict. I am a recovering addict because I was addicted to alcohol. I am a recovering alcoholic because I was addicted to alcohol. I am a recovering addict because I was addicted to alcohol\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\").cuda()\n",
    "\n",
    "input_text = \"Hello, my name is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79fc5b",
   "metadata": {},
   "source": [
    "2: Testing Falcon-7B-instruct (which looked better from a standart-input-prompt perspective.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21963a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 1.13kB [00:00, 3.19MB/s]\n",
      "Downloading tokenizer.json: 2.73MB [00:00, 154MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 964kB/s]\n",
      "Downloading config.json: 1.05kB [00:00, 3.17MB/s]\n",
      "Downloading (…)figuration_falcon.py: 7.16kB [00:00, 20.7MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Downloading modeling_falcon.py: 56.9kB [00:00, 119MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"userLocale\"'), PosixPath('\"/root/.vscode-server/cli/servers/Stable-cb0c47c0cfaad0757385834bd89d410c78a856c0/server/out/nls.messages.json\",\"locale\"'), PosixPath('{}}'), PosixPath('\"en\",\"osLocale\"'), PosixPath('\"en\",\"availableLanguages\"'), PosixPath('\"en\",\"resolvedLanguage\"'), PosixPath('\"en\",\"defaultMessagesFile\"')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /storage/venv/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /storage/venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)fetensors.index.json: 17.7kB [00:00, 34.8MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.95G/9.95G [00:43<00:00, 231MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 4.48G/4.48G [00:19<00:00, 235MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:02<00:00, 31.26s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]\n",
      "Downloading generation_config.json: 100%|██████████| 117/117 [00:00<00:00, 1.07MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum physics in simple terms.\n",
      "Quantum physics is a branch of physics that studies the behavior of tiny particles, like electrons and quarks, and how they interact with each other and with their environment. It's based on the idea that particles can exist in multiple states or locations at the same time until they're measured or observed. This is in contrast to classical physics, where objects are considered to have a definite position and momentum until an event occurs. In quantum physics, probabilities of particles' locations and behaviors are represented by wave functions, and interactions between particles are described by mathematical formulas involving matrices. These concepts are often counterintuitive and require significant mathematical sophistication.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with custom code (Flash Attention is not used by default on V100/older setups)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"  # or just .cuda() if single-GPU\n",
    ")\n",
    "\n",
    "# Inference\n",
    "prompt = \"Explain quantum physics in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda316e",
   "metadata": {},
   "source": [
    "After testing the models in-general we tried long to design a prompt that makes the small flcon model do what we want ... but the generated query was never really of high quality.\n",
    "\n",
    "Step 1: Loading the model once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c17127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "CACHE_DIR = \"/storage/courses/ds_workflow/car-web-scraping/model_cache\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR,)\n",
    "\n",
    "# Load model with custom code (Flash Attention is not used by default on V100/older setups)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    device_map=\"auto\"  # or just .cuda() if single-GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b7b6d",
   "metadata": {},
   "source": [
    "Step 2: Generating a query that is supposed to match a specific vehicle from our .yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592e9444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a car dealer interested in a car. You want to know from a chatbot what car could match your description.\n",
      "The search query shall be designed to match vaguely this result: \n",
      "\n",
      "    Category: Estate, 5 door\n",
      "    Colour: red\n",
      "    Emission class: EURO 5\n",
      "    Engine type: Diesel\n",
      "    Federal Motor Transport Authority (KBA) Key Manufacturer: '4136'\n",
      "    Federal Motor Transport Authority (KBA) Key Type: AMY\n",
      "    First registration: '01.2011'\n",
      "    Fuel type: Diesel\n",
      "    Location: D-76\n",
      "    Power output: 125 KW / 170 PS\n",
      "    Read mileage: 268.300 Kilometres\n",
      "    Supplier: 'Subsidiary/authorised dealer of the brand(s): Audi'\n",
      "    Total number of owners: 1 Owner\n",
      "    Transmission: 6-gear manual transmission\n",
      "    Vehicle Identification No.: ZAR93900007******\n",
      "    Vehicle release: Release of the vehicle will take place at the earliest 5 working\n",
      "      days following receipt of payment.\n",
      "\n",
      "Please create a realistic search query that doesn't contain every detail but only a few of the provided ones.\n",
      "Don't use exact numeric details but spans (i.e.: less than 300.000 Kilometres mileage).\n",
      "Only choose a few sensible details to include them in the query. Make the query multiple sentences long.\n",
      "Query of a car dealer:\n",
      "I'm interested in an estate, 5-door, red, 2011 Audi A4 diesel, 'AMY' key type, category 'Estate', location 'D-76', power output of 125 KW / 170 PS, mileage read up to 268.300 kilometers, transmission '6-gear manual', vehicle identification no. ZAR93900007******, and release of the vehicle within 5 working days after payment.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "prompt = \"\"\"\n",
    "You are a car dealer interested in a car. You want to know from a chatbot what car could match your description.\n",
    "The search query shall be designed to match vaguely this result: \n",
    "\n",
    "    Category: Estate, 5 door\n",
    "    Colour: red\n",
    "    Emission class: EURO 5\n",
    "    Engine type: Diesel\n",
    "    Federal Motor Transport Authority (KBA) Key Manufacturer: '4136'\n",
    "    Federal Motor Transport Authority (KBA) Key Type: AMY\n",
    "    First registration: '01.2011'\n",
    "    Fuel type: Diesel\n",
    "    Location: D-76\n",
    "    Power output: 125 KW / 170 PS\n",
    "    Read mileage: 268.300 Kilometres\n",
    "    Supplier: 'Subsidiary/authorised dealer of the brand(s): Audi'\n",
    "    Total number of owners: 1 Owner\n",
    "    Transmission: 6-gear manual transmission\n",
    "    Vehicle Identification No.: ZAR93900007******\n",
    "    Vehicle release: Release of the vehicle will take place at the earliest 5 working\n",
    "      days following receipt of payment.\n",
    "\n",
    "Please create a realistic search query that doesn't contain every detail but only a few of the provided ones.\n",
    "Don't use exact numeric details but spans (i.e.: less than 300.000 Kilometres mileage).\n",
    "Only choose a few sensible details to include them in the query. Make the query multiple sentences long.\n",
    "Query of a car dealer:\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a13a57",
   "metadata": {},
   "source": [
    "#### Side-Note: Requirements:\n",
    "\n",
    "**Core compatible versions**  \n",
    "torch==2.0.1+cu118     # Stable and compatible with CUDA 11.8  \n",
    "transformers==4.30.2   # Works with older models without Triton/FlashAttention2  \n",
    "accelerate==0.21.0     # Compatible with this transformers version  \n",
    "sentencepiece==0.1.99  # Required for some tokenizer models  \n",
    "scipy==1.11.4  \n",
    "numpy==1.24.4  \n",
    "tokenizers==0.13.3  \n",
    "datasets==2.14.5  \n",
    "peft==0.4.0            # Optional, for PEFT/LoRA if needed  \n",
    "bitsandbytes==0.39.0   # Last version that can be compiled with CUDA 11.8 (optional)\n",
    "\n",
    "**Hugging Face Hub**  \n",
    "huggingface-hub==0.17.3  \n",
    "\n",
    "**Optional utilities**  \n",
    "tqdm==4.66.1  \n",
    "einops==0.6.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
