{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Data Cleansing Pipeline\n",
    "\n",
    "This notebook processes vehicle data from `vehicles_data.json` through a cleaning pipeline. We used to have multiple scripts for different cleaning steps that were all combined in here in a sensible order. The final result will be `vehicles_info.yaml` where the scraped vehicle information is cleaned, properly translated (as even the english version of the car selling website did still show german words), and token-limited (for BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries and set up logging for the data pre-processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Iterable, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import yaml\n",
    "from langdetect import detect, LangDetectException\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Clear existing handlers\n",
    "if logger.handlers:\n",
    "    logger.handlers.clear()\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Original Data\n",
    "\n",
    "Load the original vehicle data from `vehicles_data.json` and perform initial validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /storage/courses/ds_workflow/car-web-scraping/data/vehicles_data.json\n",
      "Loaded 2600 vehicle entries\n",
      "Sample entry URL: https://autobid.de/en/item/mini-cooper-s-cabrio-3111666/details\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "base_dir = os.path.abspath(os.path.join('../../data'))\n",
    "input_file = os.path.join(base_dir, \"vehicles_data.json\")\n",
    "output_file = os.path.join(base_dir, \"vehicles_info.yaml\")\n",
    "\n",
    "# Load original data\n",
    "print(f\"Loading data from: {input_file}\")\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as read_file:\n",
    "    original_data = json.load(read_file)\n",
    "\n",
    "if not original_data:\n",
    "    raise ValueError(\"The provided file is empty—nothing to clean.\")\n",
    "\n",
    "print(f\"Loaded {len(original_data)} vehicle entries\")\n",
    "print(f\"Sample entry URL: {list(original_data.keys())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 1: Data Cleaning and Deduplication\n",
    "\n",
    "Clean the data by removing duplicate phrases from details_text that already appear in details_list or information_dict.  \n",
    "Also scraped items that doesn't contain all information fields that we expect are skipped, as sometimes not only vehicles are on sale but also vehicle parts that don't have all the information we'd expect. (And we don't want them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1 Complete:\n",
      "- Processed: 2510 entries\n",
      "- Skipped: 90 incomplete entries\n",
      "- Modified: 514 entries\n"
     ]
    }
   ],
   "source": [
    "def _dedupe_details_text(details_text: str,\n",
    "                         details_list: Iterable[str],\n",
    "                         info_values: Iterable[str]) -> str:\n",
    "    \"\"\"Remove duplicate phrases from details_text that appear in details_list or info_values.\"\"\"\n",
    "    phrases = {p.lower().strip() for p in details_list if p.strip()}\n",
    "    phrases.update(str(v).lower().strip() for v in info_values if v)\n",
    "\n",
    "    for phrase in sorted(phrases, key=len, reverse=True):\n",
    "        if not phrase:\n",
    "            continue\n",
    "        pattern = re.compile(rf\"\\b{re.escape(phrase)}\\b[.,;:]?\", re.IGNORECASE)\n",
    "        details_text = pattern.sub(\"\", details_text)\n",
    "\n",
    "    # Remove extra whitespace and punctuation\n",
    "    details_text = re.sub(r\"\\s{2,}\", \" \", details_text)\n",
    "    details_text = re.sub(r\"[;,]{2,}\", \";\", details_text)\n",
    "    details_text = re.sub(r\"^[\\s;,.]+|[\\s;,.]+$\", \"\", details_text)\n",
    "\n",
    "    return details_text.strip()\n",
    "\n",
    "# Apply deduplication\n",
    "cleaned_data = {}\n",
    "skipped_count = 0\n",
    "modified_count = 0\n",
    "\n",
    "for url, car_dict in original_data.items():\n",
    "    details_list = car_dict.get(\"details_list\", [])\n",
    "    info_dict = car_dict.get(\"information_dict\", {})\n",
    "    details_text = car_dict.get(\"details_text\", \"\")\n",
    "\n",
    "    # Skip incomplete rows\n",
    "    if not (all(details_list) and all(info_dict.values()) and details_text):\n",
    "        # logger.warning(\"Skipped incomplete entry for %s\", url)\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    cleaned_text = _dedupe_details_text(\n",
    "        details_text,\n",
    "        details_list,\n",
    "        info_dict.values()\n",
    "    )\n",
    "\n",
    "    if cleaned_text != details_text:\n",
    "        # logger.info(\"Modified details_text for %s\", url)\n",
    "        modified_count += 1\n",
    "\n",
    "    cleaned_entry = dict(car_dict)\n",
    "    cleaned_entry[\"details_text\"] = cleaned_text\n",
    "    cleaned_data[url] = cleaned_entry\n",
    "\n",
    "print(f\"\\nStep 1 Complete:\")\n",
    "print(f\"- Processed: {len(cleaned_data)} entries\")\n",
    "print(f\"- Skipped: {skipped_count} incomplete entries\")\n",
    "print(f\"- Modified: {modified_count} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 2: Text Translation\n",
    "\n",
    "Translate the German texts that are sometimes in the details to English using Google Translate.  \n",
    "For some reason this sometimes skipps texts so we still get the german version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2 Complete:\n",
      "- Processed: 2510 entries\n",
      "- Translated: 1069 entries\n"
     ]
    }
   ],
   "source": [
    "# Translation constants\n",
    "_MAX_CHARS_PER_REQUEST = 4500  # Google free web translate limit\n",
    "\n",
    "def _translate(text: str) -> str:\n",
    "    \"\"\"Translate text to English using Google Translate.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "        except LangDetectException:\n",
    "            lang = \"unknown\"\n",
    "        if lang == \"en\":\n",
    "            return text\n",
    "\n",
    "        translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
    "\n",
    "        if len(text) <= _MAX_CHARS_PER_REQUEST:\n",
    "            return translator.translate(text)\n",
    "\n",
    "        # Chunk long passages\n",
    "        chunks = [\n",
    "            text[i : i + _MAX_CHARS_PER_REQUEST]\n",
    "            for i in range(0, len(text), _MAX_CHARS_PER_REQUEST)\n",
    "        ]\n",
    "        translated_chunks = []\n",
    "        for chunk in chunks:\n",
    "            translated_chunks.append(translator.translate(chunk))\n",
    "        return \"\".join(translated_chunks)\n",
    "\n",
    "    except Exception as exc:\n",
    "        logger.error(\"Google translation failed: %s – returning original text\", exc)\n",
    "        return text\n",
    "\n",
    "# Apply translation\n",
    "translated_data = {}\n",
    "translation_count = 0\n",
    "\n",
    "for url, car in cleaned_data.items():\n",
    "    translated_entry = dict(car)\n",
    "    \n",
    "    free_text = car.get(\"details_text\", \"\")\n",
    "    if not free_text:\n",
    "        translated_data[url] = translated_entry\n",
    "        continue\n",
    "\n",
    "    translated_text = _translate(free_text)\n",
    "    translated_entry[\"details_text\"] = translated_text\n",
    "\n",
    "    if translated_text != free_text:\n",
    "        # logger.info(\"Translated: %s\", url)\n",
    "        translation_count += 1\n",
    "\n",
    "    translated_data[url] = translated_entry\n",
    "\n",
    "print(f\"\\nStep 2 Complete:\")\n",
    "print(f\"- Processed: {len(translated_data)} entries\")\n",
    "print(f\"- Translated: {translation_count} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 3: Token Limit Enforcement\n",
    "\n",
    "BERT has a Token Limit of 512 Tokens. Those Tokens aren't allowed to be exceeded by the combination of vehicle information and query together. So we decided that 400 tokens at maximum for the vehicle information might work for us and stripped the vehicle details (`details_text`) for some cars. This will leave us with 109 tokens of legnth for the search query (plus 3 tokens for start, separation and stop BERT uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Applying token limit of 400 tokens...\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "TOKEN_LIMIT = 400  # Set your desired token limit\n",
    "\n",
    "def extract_non_details_text(vehicle_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract details_list and information_dict text (parts that won't be cut).\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if 'details_list' in vehicle_data:\n",
    "        text_parts.extend(vehicle_data['details_list'])\n",
    "    \n",
    "    if 'information_dict' in vehicle_data:\n",
    "        for key, value in vehicle_data['information_dict'].items():\n",
    "            text_parts.append(f\"{key}: {value}\")\n",
    "    \n",
    "    return ' '.join(text_parts)\n",
    "\n",
    "def get_total_token_count(vehicle_data: Dict[str, Any], tokenizer) -> int:\n",
    "    \"\"\"Get total token count for a vehicle listing.\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if 'details_text' in vehicle_data:\n",
    "        text_parts.append(vehicle_data['details_text'])\n",
    "    \n",
    "    if 'details_list' in vehicle_data:\n",
    "        text_parts.extend(vehicle_data['details_list'])\n",
    "    \n",
    "    if 'information_dict' in vehicle_data:\n",
    "        for key, value in vehicle_data['information_dict'].items():\n",
    "            text_parts.append(f\"{key}: {value}\")\n",
    "    \n",
    "    full_text = ' '.join(text_parts)\n",
    "    return len(tokenizer.encode(full_text, add_special_tokens=False))\n",
    "\n",
    "def truncate_by_sentences(details_text: str, fixed_text: str, token_limit: int, tokenizer) -> str:\n",
    "    \"\"\"Truncate details_text by complete sentences.\"\"\"\n",
    "    if not details_text:\n",
    "        return details_text\n",
    "    \n",
    "    # Find sentence separators (. and ;)\n",
    "    sentence_pattern = r'[.;]'\n",
    "    sentences = re.split(sentence_pattern, details_text)\n",
    "    \n",
    "    # If no sentence separators found, return original text for word-based truncation\n",
    "    if len(sentences) <= 1:\n",
    "        return details_text\n",
    "    \n",
    "    # Rebuild sentences with their separators\n",
    "    separators = re.findall(sentence_pattern, details_text)\n",
    "    reconstructed_sentences = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences[:-1]):\n",
    "        if i < len(separators):\n",
    "            reconstructed_sentences.append(sentence + separators[i])\n",
    "    \n",
    "    # Add last sentence if it exists and doesn't end with separator\n",
    "    if sentences[-1].strip():\n",
    "        reconstructed_sentences.append(sentences[-1])\n",
    "    \n",
    "    # Find the longest valid truncation\n",
    "    truncated_details = \"\"\n",
    "    for sentence in reconstructed_sentences:\n",
    "        candidate_details = truncated_details + sentence\n",
    "        candidate_full_text = ' '.join([candidate_details, fixed_text]).strip()\n",
    "        \n",
    "        if len(tokenizer.encode(candidate_full_text, add_special_tokens=False)) <= token_limit:\n",
    "            truncated_details = candidate_details\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return truncated_details.strip()\n",
    "\n",
    "def truncate_by_words(details_text: str, fixed_text: str, token_limit: int, tokenizer) -> str:\n",
    "    \"\"\"Truncate details_text by 10-word chunks.\"\"\"\n",
    "    if not details_text:\n",
    "        return details_text\n",
    "    \n",
    "    words = details_text.split()\n",
    "    truncated_details = \"\"\n",
    "    \n",
    "    # Try chunks of 10 words\n",
    "    for i in range(0, len(words), 10):\n",
    "        candidate_words = words[:i + 10]\n",
    "        candidate_details = ' '.join(candidate_words)\n",
    "        candidate_full_text = ' '.join([candidate_details, fixed_text]).strip()\n",
    "        \n",
    "        if len(tokenizer.encode(candidate_full_text, add_special_tokens=False)) <= token_limit:\n",
    "            truncated_details = candidate_details\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # If no 10-word chunk fits, try word by word\n",
    "    if not truncated_details:\n",
    "        for i in range(1, len(words)):\n",
    "            candidate_details = ' '.join(words[:i])\n",
    "            candidate_full_text = ' '.join([candidate_details, fixed_text]).strip()\n",
    "            \n",
    "            if len(tokenizer.encode(candidate_full_text, add_special_tokens=False)) <= token_limit:\n",
    "                truncated_details = candidate_details\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return truncated_details\n",
    "\n",
    "def truncate_vehicle_text(vehicle_data: Dict[str, Any], token_limit: int, tokenizer) -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"Truncate vehicle text to stay within token limit.\"\"\"\n",
    "    # Check if truncation is needed\n",
    "    current_tokens = get_total_token_count(vehicle_data, tokenizer)\n",
    "    \n",
    "    if current_tokens <= token_limit:\n",
    "        return vehicle_data, False\n",
    "    \n",
    "    # Get the fixed parts (details_list and information_dict)\n",
    "    fixed_text = extract_non_details_text(vehicle_data)\n",
    "    \n",
    "    # Check if fixed parts already exceed limit\n",
    "    fixed_tokens = len(tokenizer.encode(fixed_text, add_special_tokens=False))\n",
    "    if fixed_tokens > token_limit:\n",
    "        print(f\"Warning: Fixed text already exceeds limit ({fixed_tokens} > {token_limit})\")\n",
    "        return vehicle_data, False\n",
    "    \n",
    "    # Get details_text\n",
    "    details_text = vehicle_data.get('details_text', '')\n",
    "    \n",
    "    # Try sentence-based truncation first\n",
    "    truncated_details = truncate_by_sentences(details_text, fixed_text, token_limit, tokenizer)\n",
    "    \n",
    "    # If sentence-based truncation didn't work well, try word-based\n",
    "    if not truncated_details or truncated_details == details_text:\n",
    "        truncated_details = truncate_by_words(details_text, fixed_text, token_limit, tokenizer)\n",
    "    \n",
    "    # Update vehicle data\n",
    "    updated_vehicle_data = vehicle_data.copy()\n",
    "    updated_vehicle_data['details_text'] = truncated_details\n",
    "    \n",
    "    return updated_vehicle_data, True\n",
    "\n",
    "print(f\"Applying token limit of {TOKEN_LIMIT} tokens...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4 Complete:\n",
      "- Processed: 2510 entries\n",
      "- Truncated: 357 entries\n",
      "- Percentage truncated: 14.2%\n",
      "- Vehicles still over limit: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply token truncation\n",
    "token_limited_data = {}\n",
    "truncated_count = 0\n",
    "over_limit_count = 0\n",
    "\n",
    "for url, data in translated_data.items():\n",
    "    processed_vehicle, was_truncated = truncate_vehicle_text(data, TOKEN_LIMIT, tokenizer)\n",
    "    token_limited_data[url] = processed_vehicle\n",
    "    \n",
    "    if was_truncated:\n",
    "        truncated_count += 1\n",
    "        original_tokens = get_total_token_count(data, tokenizer)\n",
    "        new_tokens = get_total_token_count(processed_vehicle, tokenizer)\n",
    "        # logger.info(f\"Truncated: {url} - Tokens: {original_tokens} -> {new_tokens}\")\n",
    "\n",
    "# Verify results\n",
    "for url, data in token_limited_data.items():\n",
    "    tokens = get_total_token_count(data, tokenizer)\n",
    "    if tokens > TOKEN_LIMIT:\n",
    "        over_limit_count += 1\n",
    "        logger.warning(f\"Still over limit: {url} ({tokens} tokens)\")\n",
    "\n",
    "print(f\"\\nStep 4 Complete:\")\n",
    "print(f\"- Processed: {len(token_limited_data)} entries\")\n",
    "print(f\"- Truncated: {truncated_count} entries\")\n",
    "print(f\"- Percentage truncated: {truncated_count/len(token_limited_data)*100:.1f}%\")\n",
    "print(f\"- Vehicles still over limit: {over_limit_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 4: Field Cleaning and Formatting\n",
    "\n",
    "Clean specific fields by:\n",
    "1. Converting German-style numbers (dots to commas) in mileage\n",
    "2. Removing Vehicle Identification No. fields\n",
    "3. Removing KBA Key fields\n",
    "\n",
    "Reason: The Vehicle Identification and KBA are individual vehicle numbers, and apart from the police, no one would search for exactly \"the car that has the number XY\" on the market. So we removed this info for our find-me-a-car model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4 Complete:\n",
      "- Processed: 2510 entries\n",
      "- Mileage format fixes: 2475\n",
      "- Removed fields: 6497\n"
     ]
    }
   ],
   "source": [
    "# Apply field cleaning\n",
    "final_data = {}\n",
    "mileage_fixes = 0\n",
    "removed_fields = 0\n",
    "\n",
    "for vehicle_url, vehicle_data in token_limited_data.items():\n",
    "    cleaned_entry = dict(vehicle_data)\n",
    "    \n",
    "    if 'information_dict' in cleaned_entry:\n",
    "        info_dict = cleaned_entry['information_dict']\n",
    "\n",
    "        # Convert German-style mileage numbers (dots to commas)\n",
    "        if 'Read mileage' in info_dict:\n",
    "            original_mileage = info_dict['Read mileage']\n",
    "            # Replace dots with commas in numbers (e.g., \"23.500\" -> \"23,500\")\n",
    "            info_dict['Read mileage'] = re.sub(r'(\\d+)\\.(\\d+)', r'\\1,\\2', original_mileage)\n",
    "            if info_dict['Read mileage'] != original_mileage:\n",
    "                mileage_fixes += 1\n",
    "\n",
    "        # Remove Vehicle Identification No. field\n",
    "        if 'Vehicle Identification No.' in info_dict:\n",
    "            del info_dict['Vehicle Identification No.']\n",
    "            removed_fields += 1\n",
    "\n",
    "        # Remove KBA Key fields (both manufacturer and type)\n",
    "        keys_to_remove = []\n",
    "        for key in info_dict.keys():\n",
    "            if 'KBA' in key and 'Key' in key:\n",
    "                keys_to_remove.append(key)\n",
    "\n",
    "        for key in keys_to_remove:\n",
    "            del info_dict[key]\n",
    "            removed_fields += 1\n",
    "\n",
    "    final_data[vehicle_url] = cleaned_entry\n",
    "\n",
    "print(f\"\\nStep 4 Complete:\")\n",
    "print(f\"- Processed: {len(final_data)} entries\")\n",
    "print(f\"- Mileage format fixes: {mileage_fixes}\")\n",
    "print(f\"- Removed fields: {removed_fields}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Results\n",
    "\n",
    "Save the fully processed data to `vehicles_info.yaml` with proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final processed data to: /storage/courses/ds_workflow/car-web-scraping/data/vehicles_info.yaml\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Input file: /storage/courses/ds_workflow/car-web-scraping/data/vehicles_data.json\n",
      "Output file: /storage/courses/ds_workflow/car-web-scraping/data/vehicles_info.yaml\n",
      "Total vehicles processed: 2510\n",
      "Token limit: 400\n",
      "\n",
      "Processing Summary:\n",
      "- Step 1 (Deduplication): 514 entries modified\n",
      "- Step 2 (Translation): 1069 entries translated\n",
      "- Step 3 (Field cleaning): 2475 mileage fixes, 6497 fields removed\n",
      "- Step 4 (Token limiting): 357 entries truncated\n",
      "\n",
      "Sample entry: https://autobid.de/en/item/mini-cooper-s-cabrio-3111666/details\n",
      "Token count: 216\n",
      "Keys: ['information_dict', 'details_list', 'details_text']\n",
      "Details text preview: Demurrage chargesStarting on 22.07.2025 the daily demurrage for this item will be 6,00 EUR net, plus...\n"
     ]
    }
   ],
   "source": [
    "# Save the final processed data\n",
    "print(f\"Saving final processed data to: {output_file}\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(final_data, f, default_flow_style=False, allow_unicode=True, sort_keys=True)\n",
    "\n",
    "print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "print(f\"Input file: {input_file}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total vehicles processed: {len(final_data)}\")\n",
    "print(f\"Token limit: {TOKEN_LIMIT}\")\n",
    "print(f\"\\nProcessing Summary:\")\n",
    "print(f\"- Step 1 (Deduplication): {modified_count} entries modified\")\n",
    "print(f\"- Step 2 (Translation): {translation_count} entries translated\")\n",
    "print(f\"- Step 3 (Field cleaning): {mileage_fixes} mileage fixes, {removed_fields} fields removed\")\n",
    "print(f\"- Step 4 (Token limiting): {truncated_count} entries truncated\")\n",
    "\n",
    "# Show a sample of the final data structure\n",
    "if final_data:\n",
    "    sample_url = list(final_data.keys())[0]\n",
    "    sample_data = final_data[sample_url]\n",
    "    sample_tokens = get_total_token_count(sample_data, tokenizer)\n",
    "    \n",
    "    print(f\"\\nSample entry: {sample_url}\")\n",
    "    print(f\"Token count: {sample_tokens}\")\n",
    "    print(f\"Keys: {list(sample_data.keys())}\")\n",
    "    if 'details_text' in sample_data:\n",
    "        details_preview = sample_data['details_text'][:100] + '...' if len(sample_data['details_text']) > 100 else sample_data['details_text']\n",
    "        print(f\"Details text preview: {details_preview}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
