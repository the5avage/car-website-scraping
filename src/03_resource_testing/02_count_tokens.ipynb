{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ace731",
   "metadata": {},
   "source": [
    "### Test: Amount of Tokens of our scraped information\n",
    "\n",
    "We want to fine-tune a BERT model, yet BERT has a limitation of 512 input tokens.\n",
    "\n",
    "After cleansing the scraped vehicle information and bringing it into a (hopefully good digestable) format for a language model, we need to find out, whether the scraped vehicle information exceeds the amount of 512 tokens.  \n",
    "\n",
    "We are planning to use the fine-tuned models as Cross-Encoder. This means that the search query will be added to the vehicle information with a `[SEP]` token in-between before getting passed to the model. So the concatenation of both, `information text + query` has to stay under the token limitation altogether.\n",
    "\n",
    "#### Test 1: Amount of vehicle information text tokens\n",
    "\n",
    "The first passage of this notebook shall test the amount of tokens of our vehicle-describing-texts using `translated_vehicles_data.yaml` as resource (which contains the cleansed vehicle information), so that we can get an impression, if we need to shorten the informational texts or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2510 vehicle listings\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and setup\n",
    "import yaml\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load vehicle data (global)\n",
    "def load_vehicle_data():\n",
    "    \"\"\"Load and parse the vehicle YAML data\"\"\"\n",
    "    with open('../../data/translated_vehicles_data.yaml', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    try:\n",
    "        data = yaml.unsafe_load(content)\n",
    "        vehicle_data = dict(data.items()) if hasattr(data, 'items') else data\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error with unsafe_load: {e}\")\n",
    "        # Fallback approach\n",
    "        cleaned_content = re.sub(r'!!python/object/apply:collections\\.defaultdict\\s*', '', content)\n",
    "        cleaned_content = re.sub(r'args:\\s*- !!python/name:builtins\\.dict\\s*\\'\\'?\\s*', '', cleaned_content)\n",
    "        data = yaml.safe_load(cleaned_content)\n",
    "        vehicle_data = data['dictitems'] if 'dictitems' in data else data\n",
    "    \n",
    "    return vehicle_data\n",
    "\n",
    "def extract_text_from_vehicle(vehicle_data):\n",
    "    \"\"\"Extract all text content from a vehicle listing\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if 'details_text' in vehicle_data:\n",
    "        text_parts.append(vehicle_data['details_text'])\n",
    "    \n",
    "    if 'details_list' in vehicle_data:\n",
    "        text_parts.extend(vehicle_data['details_list'])\n",
    "    \n",
    "    if 'information_dict' in vehicle_data:\n",
    "        for key, value in vehicle_data['information_dict'].items():\n",
    "            text_parts.append(f\"{key}: {value}\")\n",
    "    \n",
    "    return ' '.join(text_parts)\n",
    "\n",
    "def analyze_token_counts(data, limit=512):\n",
    "    \"\"\"Analyze token counts for all vehicles\"\"\"\n",
    "    results = []\n",
    "    over_limit_count = 0\n",
    "    \n",
    "    for url, vehicle_data in data.items():\n",
    "        text = extract_text_from_vehicle(vehicle_data)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        is_over_limit = token_count > limit\n",
    "        if is_over_limit:\n",
    "            over_limit_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            'url': url,\n",
    "            'token_count': token_count,\n",
    "            'over_limit': is_over_limit,\n",
    "            'text_preview': text[:100] + \"...\" if len(text) > 100 else text\n",
    "        })\n",
    "    \n",
    "    return results, over_limit_count\n",
    "\n",
    "def check_token_limits(data, limits=[400, 425, 450]):\n",
    "    \"\"\"Check how many texts exceed different token limits\"\"\"\n",
    "    results = {limit: 0 for limit in limits}\n",
    "    total_count = len(data)\n",
    "    \n",
    "    print(f\"Checking {total_count} vehicle listings against token limits: {limits}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for url, vehicle_data in data.items():\n",
    "        text = extract_text_from_vehicle(vehicle_data)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        token_count = len(tokens)\n",
    "        \n",
    "        for limit in limits:\n",
    "            if token_count > limit:\n",
    "                results[limit] += 1\n",
    "    \n",
    "    # Print results\n",
    "    for limit in limits:\n",
    "        over_limit = results[limit]\n",
    "        percentage = (over_limit / total_count) * 100\n",
    "        print(f\"Token limit {limit:3d}: {over_limit:3d} over limit ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load vehicle data once\n",
    "vehicle_data = load_vehicle_data()\n",
    "print(f\"Loaded {len(vehicle_data)} vehicle listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "analyze_512_tokens",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BERT Token Analysis Summary (512 tokens) ===\n",
      "Total vehicles analyzed: 2510\n",
      "Vehicles over 512 token limit: 141\n",
      "Percentage over limit: 5.6%\n",
      "\n",
      "=== Vehicles Exceeding 512 Token Limit ===\n",
      "URL: https://autobid.de/en/item/audi-a1-allstreet-35-tfsi-s-tronic-virtual-r-kam-17-3108334/details\n",
      "Token count: 778\n",
      "Excess tokens: 266\n",
      "Preview: Highlights:\n",
      "*Reversing camera*; Assistance systems:\n",
      "*Audi Pre Sense Front for adaptive speed assista...\n",
      "--------------------------------------------------------------------------------\n",
      "URL: https://autobid.de/en/item/audi-a3-sportback-2-0-tdi-s-tronic-ambition-3110005/details\n",
      "Token count: 517\n",
      "Excess tokens: 5\n",
      "Preview: Assistance package (parking aid at the front and back, acoustically and optically with a selective d...\n",
      "--------------------------------------------------------------------------------\n",
      "URL: https://autobid.de/en/item/audi-a4-allroad-2-0-tfsi-quattro-3105849/details\n",
      "Token count: 619\n",
      "Excess tokens: 107\n",
      "Preview: Audi Drive Select\n",
      "Equipment package: aluminum look/elements (roof rail chrome-plated)\n",
      "CD changer and...\n",
      "--------------------------------------------------------------------------------\n",
      "URL: https://autobid.de/en/item/audi-a4-avant-35-tdi-s-tronic-advanced-3107526/details\n",
      "Token count: 624\n",
      "Excess tokens: 112\n",
      "Preview: Acoustic glass door windows in front\n",
      "Parking assistance package (driving assistance system: parking ...\n",
      "--------------------------------------------------------------------------------\n",
      "URL: https://autobid.de/en/item/audi-a4-avant-40-tdi-mild-hybrid-quattro-s-tronic-s-line-3109635/details\n",
      "Token count: 700\n",
      "Excess tokens: 188\n",
      "Preview: Parking assistance package City with assistance package (assistant package city, driving assistance ...\n",
      "--------------------------------------------------------------------------------\n",
      "... and 136 more vehicles\n"
     ]
    }
   ],
   "source": [
    "# Analyze token counts with 512 token limit\n",
    "results, over_limit_count = analyze_token_counts(vehicle_data, limit=512)\n",
    "\n",
    "# Print summary\n",
    "total_vehicles = len(results)\n",
    "print(f\"=== BERT Token Analysis Summary (512 tokens) ===\")\n",
    "print(f\"Total vehicles analyzed: {total_vehicles}\")\n",
    "print(f\"Vehicles over 512 token limit: {over_limit_count}\")\n",
    "print(f\"Percentage over limit: {over_limit_count/total_vehicles*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Show vehicles that exceed the limit\n",
    "over_limit_vehicles = [r for r in results if r['over_limit']]\n",
    "if over_limit_vehicles:\n",
    "    print(\"=== Vehicles Exceeding 512 Token Limit ===\")\n",
    "    for vehicle in over_limit_vehicles[:5]:  # Show first 5 for brevity\n",
    "        print(f\"URL: {vehicle['url']}\")\n",
    "        print(f\"Token count: {vehicle['token_count']}\")\n",
    "        print(f\"Excess tokens: {vehicle['token_count'] - 512}\")\n",
    "        print(f\"Preview: {vehicle['text_preview']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if len(over_limit_vehicles) > 5:\n",
    "        print(f\"... and {len(over_limit_vehicles) - 5} more vehicles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7a2a9",
   "metadata": {},
   "source": [
    "As the result is that 141 vehicles out of 2510 are already over the token limit just given the informational text, the idea is to shorten these entries. But before we do that, we need an impression of what our real token limit for the vehicle information should be. Therefore we need a limit for the queries and an idea how long we want to allow the queries we generate to become. The tokens of an example query are going to be counted below to get a rough idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "query_token_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: I am looking for a grey, two-door small car whose first registration is no earlier than 2020, makes vetween 100 kW and 120 kW from a petrol engine, and shows under 30 000 km on the odometer.\n",
      "\n",
      "Query token count: 47\n",
      "Query tokens: [1045, 2572, 2559, 2005, 1037, 4462, 1010, 2048, 1011, 2341, 2235, 2482, 3005, 2034, 8819, 2003, 2053, 3041, 2084, 12609, 1010, 3084, 29525, 28394, 2078, 2531, 6448, 1998, 6036, 6448, 2013, 1037, 17141, 3194, 1010, 1998, 3065, 2104, 2382, 2199, 2463, 2006, 1996, 1051, 26173, 3334, 1012]\n",
      "Decoded tokens: ['i', 'am', 'looking', 'for', 'a', 'grey', ',', 'two', '-', 'door', 'small', 'car', 'whose', 'first', 'registration', 'is', 'no', 'earlier', 'than', '2020', ',', 'makes', 'vet', '##wee', '##n', '100', 'kw', 'and', '120', 'kw', 'from', 'a', 'petrol', 'engine', ',', 'and', 'shows', 'under', '30', '000', 'km', 'on', 'the', 'o', '##dome', '##ter', '.']\n",
      "\n",
      "For cross-encoder setup:\n",
      "Query tokens: 47\n",
      "Special tokens ([CLS], [SEP], [SEP]): 3\n",
      "Total query overhead: 50\n",
      "Available tokens for document: 462\n",
      "\n",
      "Actual cross-encoder tokens (query + special tokens): 50\n",
      "Recommended document token limit: 462\n"
     ]
    }
   ],
   "source": [
    "# Example query analysis\n",
    "query = \"I am looking for a grey, two-door small car whose first registration is no earlier than 2020, makes vetween 100 kW and 120 kW from a petrol engine, and shows under 30 000 km on the odometer.\"\n",
    "\n",
    "# Count tokens for the query\n",
    "query_tokens = tokenizer.encode(query, add_special_tokens=False)\n",
    "query_token_count = len(query_tokens)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nQuery token count: {query_token_count}\")\n",
    "print(f\"Query tokens: {query_tokens}\")\n",
    "print(f\"Decoded tokens: {tokenizer.convert_ids_to_tokens(query_tokens)}\")\n",
    "\n",
    "# For cross-encoder, account for special tokens\n",
    "special_tokens_count = 3  # [CLS] + [SEP] + [SEP]\n",
    "total_query_overhead = query_token_count + special_tokens_count\n",
    "\n",
    "print(f\"\\nFor cross-encoder setup:\")\n",
    "print(f\"Query tokens: {query_token_count}\")\n",
    "print(f\"Special tokens ([CLS], [SEP], [SEP]): {special_tokens_count}\")\n",
    "print(f\"Total query overhead: {total_query_overhead}\")\n",
    "print(f\"Available tokens for document: {512 - total_query_overhead}\")\n",
    "\n",
    "# Test with actual cross-encoder format\n",
    "def test_cross_encoder_format(query, document_text=\"\"):\n",
    "    \"\"\"Test the actual token count for cross-encoder format\"\"\"\n",
    "    encoded = tokenizer.encode(query, document_text, add_special_tokens=True, truncation=False)\n",
    "    return len(encoded)\n",
    "\n",
    "# Test with empty document\n",
    "cross_encoder_tokens = test_cross_encoder_format(query, \"\")\n",
    "print(f\"\\nActual cross-encoder tokens (query + special tokens): {cross_encoder_tokens}\")\n",
    "\n",
    "# Recommendation for document token limit\n",
    "recommended_doc_limit = 512 - cross_encoder_tokens\n",
    "print(f\"Recommended document token limit: {recommended_doc_limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "check_lower_limits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 2510 vehicle listings against token limits: [400, 425, 450]\n",
      "============================================================\n",
      "Token limit 400: 358 over limit ( 14.3%)\n",
      "Token limit 425: 283 over limit ( 11.3%)\n",
      "Token limit 450: 234 over limit (  9.3%)\n"
     ]
    }
   ],
   "source": [
    "# Check how many vehicles exceed lower token limits (for cross-encoder usage)\n",
    "results = check_token_limits(vehicle_data, [400, 425, 450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901545d",
   "metadata": {},
   "source": [
    "Result of this analysis:\n",
    "\n",
    "Cutting the entries of `translated_vehicles_data.yaml` to not exceed something between 400 to 450 tokens seems sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fadf59",
   "metadata": {},
   "source": [
    "#### Test 2: Amount of query tokens\n",
    "\n",
    "After multiple questions with different wordings have been generated for us by ChatGPT, we can check the amount of tokens these search queries have.\n",
    "Different types of queries have been grouped in different batches. One example have been pushed to gitlab: `car_match_questions_batch1.zip`.\n",
    "\n",
    "However ... this was just a first run of using LLM generated queries and rating them, and we tried to tell ChatGPT to use different styles for the queries in other batches later on, that look more like a plausible choice of words when searching for a car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ccb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 998 unique queries from the zip archive.\n",
      "0 out of 998 queries exceed 109 tokens.\n",
      "\n",
      "The longest query is 42 tokens long.\n",
      "Query text:\n",
      "With 6 or more speeds, fitted with rear parking sensors, first registered between 2015 and 2022, making between 59 kW and 140 kW, showing between 70900 km and 124300 km.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Path to an example batch file\n",
    "zip_path = '../../data/car_match_questions_batch1.zip'\n",
    "\n",
    "# Token limit to check\n",
    "token_limit = 109 # If our vehicle information limit is 400 tokens\n",
    "\n",
    "# Set to store all unique queries\n",
    "queries = set()\n",
    "\n",
    "# Read queries directly from zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "    for file_name in zipf.namelist():\n",
    "        if file_name.startswith(\"questions_part\") and file_name.endswith(\".json\"):\n",
    "            with zipf.open(file_name) as f:\n",
    "                data = json.load(f)\n",
    "                queries.update(data.keys())\n",
    "\n",
    "print(f\"Loaded {len(queries)} unique queries from the zip archive.\")\n",
    "\n",
    "# Analyze token lengths\n",
    "over_limit_count = 0\n",
    "query_token_lengths = []\n",
    "max_token_count = 0\n",
    "longest_query = \"\"\n",
    "\n",
    "for query in queries:\n",
    "    tokens = tokenizer.encode(query, add_special_tokens=True)\n",
    "    token_count = len(tokens)\n",
    "    query_token_lengths.append((query, token_count))\n",
    "    if token_count > token_limit:\n",
    "        over_limit_count += 1\n",
    "    if token_count > max_token_count:\n",
    "        max_token_count = token_count\n",
    "        longest_query = query\n",
    "\n",
    "print(f\"{over_limit_count} out of {len(queries)} queries exceed {token_limit} tokens.\")\n",
    "print(f\"\\nThe longest query is {max_token_count} tokens long.\")\n",
    "print(\"Query text:\")\n",
    "print(longest_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
