{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, AutoConfig\n",
    ")\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import uuid\n",
    "import wandb\n",
    "wandb.login(key=\"your-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2867b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vehicles_path = \"../../data/train_vehicles_info.yaml\"\n",
    "train_queries_path = \"../../data/train_generated_questions.json\"\n",
    "test_vehicles_path = \"../../data/test_vehicles_info.yaml\"\n",
    "test_queries_path = \"../../data/test_generated_questions.json\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af82ce3",
   "metadata": {},
   "source": [
    "## Settings for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the fine-tuning process\"\"\"\n",
    "    model_name: str = \"roberta-base\"  # or \"microsoft/deberta-v3-base\"\n",
    "    max_length: int = 512\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 16\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.01\n",
    "    output_dir: str = \"./roberta\"\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# Start main W&B run manually\n",
    "random_id = uuid.uuid4().hex[:6] # e.g., 'a7c3d9'\n",
    "wandb_group = f\"5-fold-cv-roberta-{random_id}\"\n",
    "wandb_project = \"vehicle-cross-encoder\"\n",
    "\n",
    "main_run = wandb.init(\n",
    "    project=wandb_project,\n",
    "    name=wandb_group + \"-main\",\n",
    "    group=wandb_group,\n",
    "    config=config.__dict__,\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "run_name = wandb.run.name\n",
    "config.output_dir = os.path.join(config.output_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDataset(Dataset):\n",
    "    \"\"\"Dataset class for vehicle-query pairs\"\"\"\n",
    "    \n",
    "    def __init__(self, data_pairs: List[Tuple[str, str, int]], tokenizer, max_length: int = 512):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query, vehicle_text, label = self.data_pairs[idx]\n",
    "        \n",
    "        # Tokenize the query-vehicle pair\n",
    "        # Following cross-encoder format: [CLS] query [SEP] vehicle_text [SEP]\n",
    "        encoding = self.tokenizer(\n",
    "            query,\n",
    "            vehicle_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleCrossEncoder:\n",
    "    \"\"\"Main class for fine-tuning cross-encoder models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def load_training_data(self, train_vehicles_file: str, train_questions_file: str, num_folds: int = 5) -> List[Tuple[List, List]]:\n",
    "        \"\"\"Load training data and return 5-fold cross-validation splits\"\"\"\n",
    "        \n",
    "        # Load training vehicle data\n",
    "        with open(train_vehicles_file, 'r', encoding='utf-8') as f:\n",
    "            vehicles_data = yaml.safe_load(f)\n",
    "\n",
    "        # Load training questions data\n",
    "        with open(train_questions_file, 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "\n",
    "        # Prepare vehicle-level data\n",
    "        vehicle_datasets = []\n",
    "        for vehicle_url, vehicle_info in vehicles_data.items():\n",
    "            vehicle_text = self._create_vehicle_description(vehicle_info)\n",
    "            if vehicle_url in questions_data:\n",
    "                questions = questions_data[vehicle_url]\n",
    "                vehicle_pairs = [(q, vehicle_text, int(label)) for q, label in questions.items()]\n",
    "                vehicle_datasets.append({'url': vehicle_url, 'pairs': vehicle_pairs})\n",
    "\n",
    "        # Prepare non-overlapping folds from all training vehicles\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        random.shuffle(vehicle_datasets)\n",
    "        \n",
    "        # Calculate fold sizes\n",
    "        total_vehicles = len(vehicle_datasets)\n",
    "        fold_size = total_vehicles // num_folds\n",
    "        remainder = total_vehicles % num_folds\n",
    "        \n",
    "        folds = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        for fold_idx in range(num_folds):\n",
    "            # Calculate fold size (distribute remainder across first few folds)\n",
    "            current_fold_size = fold_size + (1 if fold_idx < remainder else 0)\n",
    "            \n",
    "            # Get validation vehicles for this fold\n",
    "            val_vehicles = vehicle_datasets[start_idx:start_idx + current_fold_size]\n",
    "            \n",
    "            # Get training vehicles (all others)\n",
    "            train_vehicles = vehicle_datasets[:start_idx] + vehicle_datasets[start_idx + current_fold_size:]\n",
    "            \n",
    "            # Convert to pairs\n",
    "            train_pairs = []\n",
    "            val_pairs = []\n",
    "            \n",
    "            for vehicle in train_vehicles:\n",
    "                train_pairs.extend(vehicle['pairs'])\n",
    "            for vehicle in val_vehicles:\n",
    "                val_pairs.extend(vehicle['pairs'])\n",
    "            \n",
    "            random.shuffle(train_pairs)\n",
    "            folds.append((train_pairs, val_pairs))\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1}: {len(train_vehicles)} train vehicles, {len(val_vehicles)} val vehicles\")\n",
    "            logger.info(f\"  → Train pairs: {len(train_pairs)}\")\n",
    "            logger.info(f\"  → Val pairs:   {len(val_pairs)}\")\n",
    "            \n",
    "            start_idx += current_fold_size\n",
    "\n",
    "        return folds\n",
    "\n",
    "    def load_test_data(self, test_vehicles_file: str, test_questions_file: str) -> List:\n",
    "        \"\"\"Load test data and return test pairs\"\"\"\n",
    "        \n",
    "        # Load test vehicle data\n",
    "        with open(test_vehicles_file, 'r', encoding='utf-8') as f:\n",
    "            vehicles_data = yaml.safe_load(f)\n",
    "\n",
    "        # Load test questions data\n",
    "        with open(test_questions_file, 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "\n",
    "        # Prepare test pairs\n",
    "        test_pairs = []\n",
    "        for vehicle_url, vehicle_info in vehicles_data.items():\n",
    "            vehicle_text = self._create_vehicle_description(vehicle_info)\n",
    "            if vehicle_url in questions_data:\n",
    "                questions = questions_data[vehicle_url]\n",
    "                vehicle_pairs = [(q, vehicle_text, int(label)) for q, label in questions.items()]\n",
    "                test_pairs.extend(vehicle_pairs)\n",
    "\n",
    "        logger.info(f\"Test set: {len(vehicles_data)} vehicles → {len(test_pairs)} pairs\")\n",
    "        \n",
    "        return test_pairs\n",
    "    \n",
    "    def _create_vehicle_description(self, vehicle_info: Dict) -> str:\n",
    "        \"\"\"Create a comprehensive vehicle description from the data\"\"\"\n",
    "        description_parts = []\n",
    "        \n",
    "        # Add information dictionary details\n",
    "        if 'information_dict' in vehicle_info:\n",
    "            info_dict = vehicle_info['information_dict']\n",
    "            for key, value in info_dict.items():\n",
    "                description_parts.append(f\"{key}: {value}\")\n",
    "        \n",
    "        # Add details list\n",
    "        if 'details_list' in vehicle_info:\n",
    "            details = \" | \".join(vehicle_info['details_list'])\n",
    "            description_parts.append(details)\n",
    "        \n",
    "        # Add details text if available\n",
    "        if 'details_text' in vehicle_info:\n",
    "            description_parts.append(vehicle_info['details_text'])\n",
    "        \n",
    "        return \" | \".join(description_parts)\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize tokenizer and model\"\"\"\n",
    "        logger.info(f\"Loading model: {self.config.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Load model configuration and modify for binary classification\n",
    "        model_config = AutoConfig.from_pretrained(self.config.model_name)\n",
    "        model_config.num_labels = 2  # Binary classification\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            config=model_config,\n",
    "            ignore_mismatched_sizes=True \n",
    "        )\n",
    "        \n",
    "        logger.info(\"Model and tokenizer loaded successfully\")\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"Compute metrics for evaluation\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    def train(self, test_pairs: List, folds: List[Tuple[List, List]]):\n",
    "        \"\"\"Train and evaluate the model using k-fold cross-validation and a held-out test set.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Model not initialized. Call initialize_model() first.\")\n",
    "        \n",
    "        all_test_results = []\n",
    "\n",
    "        for fold_idx, (train_pairs, val_pairs) in enumerate(folds):\n",
    "            logger.info(f\"\\n====== Fold {fold_idx + 1} / {len(folds)} ======\")\n",
    "            logger.info(f\"Train size: {len(train_pairs)} | Validation size: {len(val_pairs)}\")\n",
    "\n",
    "            # Create datasets\n",
    "            train_dataset = VehicleDataset(train_pairs, self.tokenizer, self.config.max_length)\n",
    "            val_dataset = VehicleDataset(val_pairs, self.tokenizer, self.config.max_length)\n",
    "            test_dataset = VehicleDataset(test_pairs, self.tokenizer, self.config.max_length)\n",
    "\n",
    "            # Set up fold-specific output directory\n",
    "            fold_output_dir = os.path.join(self.config.output_dir, f\"fold_{fold_idx + 1}\")\n",
    "            os.makedirs(fold_output_dir, exist_ok=True)\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=fold_output_dir,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                per_device_eval_batch_size=self.config.batch_size,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                warmup_steps=self.config.warmup_steps,\n",
    "                logging_steps=self.config.logging_steps,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=self.config.eval_steps,\n",
    "                save_steps=self.config.save_steps,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"f1\",\n",
    "                greater_is_better=True,\n",
    "                save_total_limit=2,\n",
    "                report_to=\"wandb\",\n",
    "                run_name=f\"{wandb_group}-fold-{fold_idx + 1}\"\n",
    "            )\n",
    "\n",
    "            fold_run = wandb.init(\n",
    "                project=wandb_project,\n",
    "                name=f\"{wandb_group}-fold-{fold_idx + 1}\",\n",
    "                group=wandb_group,\n",
    "                config=config.__dict__,\n",
    "                reinit=True\n",
    "            )\n",
    "\n",
    "            # Reinitialize trainer (model stays the same unless you want to reinit for each fold)\n",
    "            self.trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=self.compute_metrics,\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            logger.info(\"Starting training...\")\n",
    "            self.trainer.train()\n",
    "\n",
    "            # Evaluate on test set\n",
    "            logger.info(\"Evaluating on held-out test set...\")\n",
    "            test_results = self.trainer.evaluate(test_dataset)\n",
    "            logger.info(f\"Fold {fold_idx + 1} test results: {test_results}\")\n",
    "            all_test_results.append(test_results)\n",
    "\n",
    "            # Save model and tokenizer\n",
    "            self.trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(fold_output_dir)\n",
    "\n",
    "            # Finish wandb fold run\n",
    "            fold_run.finish()\n",
    "\n",
    "        logger.info(\"All folds completed.\")\n",
    "        main_run.finish() # finish wandb main run\n",
    "        return all_test_results\n",
    "\n",
    "    def predict(self, query: str, vehicle_text: str) -> Tuple[float, int]:\n",
    "        \"\"\"Make a prediction for a query-vehicle pair\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Model not initialized or trained.\")\n",
    "        \n",
    "        # Tokenize input\n",
    "        encoding = self.tokenizer(\n",
    "            query,\n",
    "            vehicle_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.config.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        # Get probability of positive class (index 1)\n",
    "        positive_prob = predictions[0][1].item()\n",
    "        predicted_label = int(positive_prob > 0.5)\n",
    "        \n",
    "        return positive_prob, predicted_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cross-encoder\n",
    "cross_encoder = VehicleCrossEncoder(config)\n",
    "\n",
    "# Load training data for cross-validation\n",
    "folds = cross_encoder.load_training_data(\n",
    "    train_vehicles_file=train_vehicles_path,\n",
    "    train_questions_file=train_queries_path\n",
    ")\n",
    "\n",
    "# Load test data for final evaluation\n",
    "test_pairs = cross_encoder.load_test_data(\n",
    "    test_vehicles_file=test_vehicles_path,\n",
    "    test_questions_file=test_queries_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f47dc8",
   "metadata": {},
   "source": [
    "**About the train / val / test split:**  \n",
    "Each set contains the complete vehicle_info x 10 corresponding queries combinations to avoid data leakage about the cars between the sets. The test set is randomly shuffled. The other two sets are not, which is why the `preview_cross_encoder_inputs()` function will show the vehicle_info x 10 corresponding queries combinations sorted for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_cross_encoder_inputs():\n",
    "    # Preview function\n",
    "    def print_preview(pairs, set_name, fold_idx=None):\n",
    "        header = f\"\\n=== Preview from {set_name.upper()} Set\"\n",
    "        if fold_idx is not None:\n",
    "            header += f\" (Fold {fold_idx + 1})\"\n",
    "        header += \" (first 10 examples) ===\\n\"\n",
    "        print(header)\n",
    "        for i, (query, vehicle_text, label) in enumerate(pairs[:10]):\n",
    "            print(f\"[{i+1}] Label: {label}\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Vehicle Text: {vehicle_text}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    # Print test set preview\n",
    "    print_preview(test_pairs, \"test\")\n",
    "\n",
    "    # Print preview for each fold\n",
    "    for fold_idx, (train_pairs, val_pairs) in enumerate(folds):\n",
    "        print_preview(train_pairs, \"train\", fold_idx=fold_idx)\n",
    "        print_preview(val_pairs, \"val\", fold_idx=fold_idx)\n",
    "\n",
    "preview_cross_encoder_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d93f00",
   "metadata": {},
   "source": [
    "Start Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71189e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "cross_encoder.initialize_model()\n",
    "\n",
    "# Train the model\n",
    "test_results = cross_encoder.train(test_pairs, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1d662",
   "metadata": {},
   "source": [
    "### Test the Model using ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_from_folds(test_pairs, threshold=0.5, num_folds: int = 5, device=None):\n",
    "    \"\"\"Load models from fold directories and evaluate ensemble on the test set.\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Load tokenizer from any fold (assume same tokenizer for all)\n",
    "    fold_dirs = [\n",
    "        os.path.join(config.output_dir, f\"fold_{i + 1}\")\n",
    "        for i in range(num_folds)\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fold_dirs[0])\n",
    "    models = []\n",
    "    for fold_dir in fold_dirs:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(fold_dir)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    # Ensemble prediction\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "    for query, vehicle_text, label in test_pairs:\n",
    "        encoding = tokenizer(query, vehicle_text, truncation=True, padding='max_length',\n",
    "                            max_length=512, return_tensors='pt').to(device)\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                output = model(**encoding)\n",
    "                prob = torch.nn.functional.softmax(output.logits, dim=-1)[0][1].item()\n",
    "                probs.append(prob)\n",
    "        avg_prob = sum(probs) / len(probs)\n",
    "        prediction = int(avg_prob >= threshold)\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_pred.append(prediction)\n",
    "        y_prob.append(avg_prob)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    # Log\n",
    "    print(\"\\n=== Final Ensemble Metrics on Held-out Test Set ===\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.capitalize()}: {v:.4f}\")\n",
    "\n",
    "    return metrics, y_true, y_pred, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b506361",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, y_true, y_pred, y_prob = evaluate_ensemble_from_folds(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_predictions(test_pairs, num_examples: int = 10, threshold: float = 0.5, num_folds: int = 5):\n",
    "    print(f\"\\n=== Ensemble Predictions on {num_examples} Random Test Examples ===\\n\")\n",
    "    \n",
    "    import random\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load fold models\n",
    "    fold_dirs = [\n",
    "        os.path.join(config.output_dir, f\"fold_{i + 1}\")\n",
    "        for i in range(num_folds)\n",
    "    ]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fold_dirs[0])\n",
    "    models = []\n",
    "    for fold_dir in fold_dirs:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(fold_dir)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    # Random sample\n",
    "    sampled = random.sample(test_pairs, num_examples)\n",
    "\n",
    "    for i, (query, vehicle_text, label) in enumerate(sampled):\n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            query,\n",
    "            vehicle_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=config.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        # Get predictions from all models\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                outputs = model(**encoding)\n",
    "                prob = torch.nn.functional.softmax(outputs.logits, dim=-1)[0][1].item()\n",
    "                probs.append(prob)\n",
    "\n",
    "        avg_prob = sum(probs) / len(probs)\n",
    "        pred = int(avg_prob >= threshold)\n",
    "\n",
    "        print(f\"[{i + 1}]\")\n",
    "        print(f\"True Label:         {'Match' if label == 1 else 'No match'}\")\n",
    "        print(f\"Predicted Label:    {'Match' if pred == 1 else 'No match'}\")\n",
    "        print(f\"Avg Match Prob:     {avg_prob:.4f}\")\n",
    "        print(f\"Individual Probs:   {[f'{p:.4f}' for p in probs]}\")\n",
    "        print(f\"Query:              {query}\")\n",
    "        print(f\"Vehicle Text:       {vehicle_text}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "show_test_predictions(test_pairs, num_examples=10, threshold=0.5, num_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_h100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
