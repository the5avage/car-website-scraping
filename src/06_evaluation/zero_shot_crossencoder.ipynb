{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afd9daa",
   "metadata": {},
   "source": [
    "### Test: RoBERTa and DeBERTa without further fine-tune as baseline\n",
    "This notebook evaluates the RoBERTa and DeBERTa models available in the sentence_transformers library for our binary classification task, deciding whether a query matches a vehicle description. As those models originally provide three outputs: entailment, contradiction and neutral score, we had to define a calculation, that is: `prediction = (a * entailment + b * neutrality + c * contradiction) > threshold` to decide whether a query is labeled as 'true' or not. Different combinations for the parameters a, b, c and the threshold are tested on the train-and-val set, and the best combination is used to make the final prediction on the test set to evaluate the models as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3593fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/venv_sentence/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CrossEncoderEvaluator:\n",
    "    \"\"\"Evaluator for pretrained CrossEncoder on vehicle matching task\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize with pretrained CrossEncoder model\"\"\"\n",
    "        self.model = CrossEncoder(model_name)\n",
    "        logger.info(f\"Loaded CrossEncoder model: {model_name}\")\n",
    "    \n",
    "    def load_test_data(self, test_vehicles_file: str, test_questions_file: str) -> List[Tuple[str, str, int]]:\n",
    "        \"\"\"Load test data and return test pairs\"\"\"\n",
    "        \n",
    "        # Load test vehicle data\n",
    "        with open(test_vehicles_file, 'r', encoding='utf-8') as f:\n",
    "            vehicles_data = yaml.safe_load(f)\n",
    "\n",
    "        # Load test questions data\n",
    "        with open(test_questions_file, 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "\n",
    "        # Prepare test pairs\n",
    "        test_pairs = []\n",
    "        for vehicle_url, vehicle_info in vehicles_data.items():\n",
    "            vehicle_text = self._create_vehicle_description(vehicle_info)\n",
    "            if vehicle_url in questions_data:\n",
    "                questions = questions_data[vehicle_url]\n",
    "                vehicle_pairs = [(q, vehicle_text, int(label)) for q, label in questions.items()]\n",
    "                test_pairs.extend(vehicle_pairs)\n",
    "\n",
    "        logger.info(f\"Test set: {len(vehicles_data)} vehicles â†’ {len(test_pairs)} pairs\")\n",
    "        \n",
    "        return test_pairs\n",
    "    \n",
    "    def _create_vehicle_description(self, vehicle_info: Dict) -> str:\n",
    "        \"\"\"Create a comprehensive vehicle description from the data\"\"\"\n",
    "        description_parts = []\n",
    "        \n",
    "        # Add information dictionary details\n",
    "        if 'information_dict' in vehicle_info:\n",
    "            info_dict = vehicle_info['information_dict']\n",
    "            for key, value in info_dict.items():\n",
    "                description_parts.append(f\"{key}: {value}\")\n",
    "        \n",
    "        # Add details list\n",
    "        if 'details_list' in vehicle_info:\n",
    "            details = \" | \".join(vehicle_info['details_list'])\n",
    "            description_parts.append(details)\n",
    "        \n",
    "        # Add details text if available\n",
    "        if 'details_text' in vehicle_info:\n",
    "            description_parts.append(vehicle_info['details_text'])\n",
    "        \n",
    "        return \" | \".join(description_parts)\n",
    "    \n",
    "    def predict_probabilities(self, test_pairs: List[Tuple[str, str, int]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get predictions from the CrossEncoder model\"\"\"\n",
    "        \n",
    "        # Prepare input pairs for the model\n",
    "        input_pairs = [(query, vehicle_text) for query, vehicle_text, _ in test_pairs]\n",
    "        true_labels = np.array([label for _, _, label in test_pairs])\n",
    "        \n",
    "        # Get predictions: [contradiction, neutral, entailment]\n",
    "        logger.info(\"Getting predictions from CrossEncoder...\")\n",
    "        predictions = self.model.predict(input_pairs, apply_softmax=True)\n",
    "        \n",
    "        logger.info(f\"Predictions shape: {predictions.shape}\")\n",
    "        logger.info(f\"Prediction sample: {predictions[0]}\")\n",
    "        \n",
    "        return predictions, true_labels\n",
    "    \n",
    "    def optimize_binary_classification(self, predictions: np.ndarray, true_labels: np.ndarray) -> Dict:\n",
    "        \"\"\"Find optimal way to combine entailment and contradiction for binary classification\"\"\"\n",
    "        \n",
    "        # Extract individual probabilities\n",
    "        contradiction_probs = predictions[:, 0]  # Index 0: contradiction\n",
    "        neutral_probs = predictions[:, 1]       # Index 1: neutral\n",
    "        entailment_probs = predictions[:, 2]    # Index 2: entailment\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_config = None\n",
    "        results = []\n",
    "        \n",
    "        # Define parameter grid for optimization\n",
    "        param_grid = {\n",
    "            'entailment_weight': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            'contradiction_weight': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, -0.2, -0.4, -0.6, -0.8, -1.0],\n",
    "            'neutral_weight': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, -0.2, -0.4, -0.6, -0.8, -1.0],\n",
    "            'threshold': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Optimizing binary classification parameters...\")\n",
    "        \n",
    "        for params in ParameterGrid(param_grid):\n",
    "            # Skip if all weights are zero\n",
    "            if params['entailment_weight'] + params['contradiction_weight'] + params['neutral_weight'] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted score\n",
    "            weighted_score = (\n",
    "                params['entailment_weight'] * entailment_probs +\n",
    "                params['contradiction_weight'] * contradiction_probs +\n",
    "                params['neutral_weight'] * neutral_probs\n",
    "            )\n",
    "            \n",
    "            # Apply threshold to get binary predictions\n",
    "            binary_predictions = (weighted_score > params['threshold']).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            f1 = f1_score(true_labels, binary_predictions)\n",
    "            accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "            precision = precision_score(true_labels, binary_predictions, zero_division=0)\n",
    "            recall = recall_score(true_labels, binary_predictions, zero_division=0)\n",
    "            \n",
    "            result = {\n",
    "                'entailment_weight': params['entailment_weight'],\n",
    "                'contradiction_weight': params['contradiction_weight'],\n",
    "                'neutral_weight': params['neutral_weight'],\n",
    "                'threshold': params['threshold'],\n",
    "                'f1': f1,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Track best F1 score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_config = result.copy()\n",
    "        \n",
    "        return best_config, results\n",
    "    \n",
    "    def evaluate_with_config(self, predictions: np.ndarray, true_labels: np.ndarray, config: Dict) -> Dict:\n",
    "        \"\"\"Evaluate using specific configuration\"\"\"\n",
    "        \n",
    "        # Extract probabilities\n",
    "        contradiction_probs = predictions[:, 0]\n",
    "        neutral_probs = predictions[:, 1]\n",
    "        entailment_probs = predictions[:, 2]\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        weighted_score = (\n",
    "            config['entailment_weight'] * entailment_probs +\n",
    "            config['contradiction_weight'] * contradiction_probs +\n",
    "            config['neutral_weight'] * neutral_probs\n",
    "        )\n",
    "        \n",
    "        # Apply threshold\n",
    "        binary_predictions = (weighted_score > config['threshold']).astype(int)\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(true_labels, binary_predictions),\n",
    "            'f1': f1_score(true_labels, binary_predictions),\n",
    "            'precision': precision_score(true_labels, binary_predictions, zero_division=0),\n",
    "            'recall': recall_score(true_labels, binary_predictions, zero_division=0),\n",
    "            'classification_report': classification_report(true_labels, binary_predictions)\n",
    "        }\n",
    "        \n",
    "        return metrics, binary_predictions, weighted_score\n",
    "    \n",
    "    def run_evaluation(self, test_vehicles_file: str, test_questions_file: str) -> Dict:\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        \n",
    "        # Load test data\n",
    "        test_pairs = self.load_test_data(test_vehicles_file, test_questions_file)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions, true_labels = self.predict_probabilities(test_pairs)\n",
    "        \n",
    "        # Optimize binary classification\n",
    "        best_config, all_results = self.optimize_binary_classification(predictions, true_labels)\n",
    "        \n",
    "        # Evaluate with best configuration\n",
    "        final_metrics, binary_predictions, weighted_scores = self.evaluate_with_config(\n",
    "            predictions, true_labels, best_config\n",
    "        )\n",
    "        \n",
    "        # Create results summary\n",
    "        results = {\n",
    "            'best_config': best_config,\n",
    "            'final_metrics': final_metrics,\n",
    "            'num_test_pairs': len(test_pairs),\n",
    "            'predictions': predictions,\n",
    "            'true_labels': true_labels,\n",
    "            'binary_predictions': binary_predictions,\n",
    "            'weighted_scores': weighted_scores,\n",
    "            'optimization_results': all_results\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def display_results(self, test_vehicles_path, test_queries_path, results, model_name=None):\n",
    "        \"\"\"Run evaluation and print results for a given evaluator.\"\"\"\n",
    "        title = model_name if model_name else \"CROSSENCODER\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"{title.upper()} EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        print(f\"\\nDataset Size: {results['num_test_pairs']} test pairs\")\n",
    "\n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        best_config = results['best_config']\n",
    "        print(f\"  Entailment Weight: {best_config['entailment_weight']:.2f}\")\n",
    "        print(f\"  Contradiction Weight: {best_config['contradiction_weight']:.2f}\")\n",
    "        print(f\"  Neutral Weight: {best_config['neutral_weight']:.2f}\")\n",
    "        print(f\"  Threshold: {best_config['threshold']:.2f}\")\n",
    "\n",
    "        print(f\"\\nFinal Metrics:\")\n",
    "        final_metrics = results['final_metrics']\n",
    "        print(f\"  Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1 Score: {final_metrics['f1']:.4f}\")\n",
    "        print(f\"  Precision: {final_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {final_metrics['recall']:.4f}\")\n",
    "\n",
    "        print(f\"\\nDetailed Classification Report:\")\n",
    "        print(final_metrics['classification_report'])\n",
    "\n",
    "        # Show some example predictions\n",
    "        print(f\"\\nExample Predictions (first 10):\")\n",
    "        test_pairs = self.load_test_data(test_vehicles_path, test_queries_path)\n",
    "        for i in range(min(10, len(test_pairs))):\n",
    "            query, vehicle_text, true_label = test_pairs[i]\n",
    "            pred_probs = results['predictions'][i]\n",
    "            binary_pred = results['binary_predictions'][i]\n",
    "            weighted_score = results['weighted_scores'][i]\n",
    "\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Query: {query[:100]}...\")\n",
    "            print(f\"  True Label: {true_label}\")\n",
    "            print(f\"  Probabilities [contradiction, neutral, entailment]: {pred_probs}\")\n",
    "            print(f\"  Weighted Score: {weighted_score:.4f}\")\n",
    "            print(f\"  Binary Prediction: {binary_pred}\")\n",
    "            print(f\"  Correct: {'âœ“' if binary_pred == true_label else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10408d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda:0\n",
      "INFO:__main__:Loaded CrossEncoder model: cross-encoder/nli-roberta-base\n",
      "INFO:sentence_transformers.cross_encoder.CrossEncoder:Use pytorch device: cuda:0\n",
      "INFO:__main__:Loaded CrossEncoder model: cross-encoder/nli-deberta-v3-base\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "test_vehicles_path = \"../../data/test_vehicles_info.yaml\"\n",
    "test_queries_path = \"../../data/test_generated_questions.json\"\n",
    "train_vehicles_path = \"../../data/train_vehicles_info.yaml\"\n",
    "train_queries_path = \"../../data/train_generated_questions.json\"\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_roberta = CrossEncoderEvaluator(model_name='cross-encoder/nli-roberta-base')\n",
    "evaluator_deberta = CrossEncoderEvaluator(model_name='cross-encoder/nli-deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a215ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Test set: 471 vehicles â†’ 4710 pairs\n",
      "INFO:__main__:Getting predictions from CrossEncoder...\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:14<00:00,  9.99it/s]\n",
      "INFO:__main__:Predictions shape: (4710, 3)\n",
      "INFO:__main__:Prediction sample: [0.01257597 0.01020252 0.9772215 ]\n",
      "INFO:__main__:Optimizing binary classification parameters...\n",
      "INFO:__main__:Test set: 82 vehicles â†’ 820 pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ROBERTA EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Dataset Size: 4710 test pairs\n",
      "\n",
      "Best Configuration:\n",
      "  Entailment Weight: 0.60\n",
      "  Contradiction Weight: -0.40\n",
      "  Neutral Weight: 1.00\n",
      "  Threshold: 0.20\n",
      "\n",
      "Final Metrics:\n",
      "  Accuracy: 0.6501\n",
      "  F1 Score: 0.7171\n",
      "  Precision: 0.6017\n",
      "  Recall: 0.8874\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.41      0.54      2356\n",
      "           1       0.60      0.89      0.72      2354\n",
      "\n",
      "    accuracy                           0.65      4710\n",
      "   macro avg       0.69      0.65      0.63      4710\n",
      "weighted avg       0.69      0.65      0.63      4710\n",
      "\n",
      "\n",
      "Example Predictions (first 10):\n",
      "\n",
      "Example 1:\n",
      "  Query: Looking for a 5-door all-terrain electric vehicle with automatic transmission, black metallic color,...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.01257597 0.01020252 0.9772215 ]\n",
      "  Weighted Score: 0.5915\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 2:\n",
      "  Query: Searching for an electric vehicle with a minimum of 6 airbags, LED headlights, and a park assistant....\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.02287783 0.00860302 0.96851915]\n",
      "  Weighted Score: 0.5806\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 3:\n",
      "  Query: In search of an electric vehicle with automatic tailgate and lane keeping system. The car should hav...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.01997288 0.01485387 0.9651733 ]\n",
      "  Weighted Score: 0.5860\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 4:\n",
      "  Query: Interested in a 5-door electric vehicle with ABS and stability program. The car should have a power ...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.01652069 0.01666562 0.9668137 ]\n",
      "  Weighted Score: 0.5901\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 5:\n",
      "  Query: Looking for an electric vehicle with cruise control and reverse camera. The car should have an autom...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.0191926  0.00977884 0.97102857]\n",
      "  Weighted Score: 0.5847\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 6:\n",
      "  Query: Searching for a petrol vehicle with a power output of less than 90 KW and mileage not exceeding 15,0...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.01109563 0.00398179 0.9849225 ]\n",
      "  Weighted Score: 0.5905\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 7:\n",
      "  Query: In search of a diesel vehicle with automatic tailgate and lane keeping system. The car should have a...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.8765581  0.00178043 0.12166138]\n",
      "  Weighted Score: -0.2758\n",
      "  Binary Prediction: 0\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 8:\n",
      "  Query: Interested in a 3-door electric vehicle with ABS and stability program. The car should have a power ...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.06051806 0.00553398 0.9339479 ]\n",
      "  Weighted Score: 0.5417\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 9:\n",
      "  Query: Looking for a 5-door all-terrain petrol vehicle with automatic transmission. The car should have a p...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.01770308 0.00561932 0.97667766]\n",
      "  Weighted Score: 0.5845\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 10:\n",
      "  Query: Searching for an electric vehicle with a minimum of 8 airbags, LED headlights, and a park assistant....\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.05341665 0.00422117 0.94236225]\n",
      "  Weighted Score: 0.5483\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n"
     ]
    }
   ],
   "source": [
    "# Display results for RoBERTa\n",
    "results_roberta = evaluator_roberta.run_evaluation(train_vehicles_path, train_queries_path)\n",
    "evaluator_roberta.display_results(test_vehicles_path, test_queries_path, results_roberta, model_name='RoBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "166fb244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Test set: 471 vehicles â†’ 4710 pairs\n",
      "INFO:__main__:Getting predictions from CrossEncoder...\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:36<00:00,  4.02it/s]\n",
      "INFO:__main__:Predictions shape: (4710, 3)\n",
      "INFO:__main__:Prediction sample: [3.5967646e-04 3.6983230e-04 9.9927050e-01]\n",
      "INFO:__main__:Optimizing binary classification parameters...\n",
      "INFO:__main__:Test set: 82 vehicles â†’ 820 pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DEBERTA EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "Dataset Size: 4710 test pairs\n",
      "\n",
      "Best Configuration:\n",
      "  Entailment Weight: 0.60\n",
      "  Contradiction Weight: -1.00\n",
      "  Neutral Weight: 1.00\n",
      "  Threshold: 0.50\n",
      "\n",
      "Final Metrics:\n",
      "  Accuracy: 0.7968\n",
      "  F1 Score: 0.8146\n",
      "  Precision: 0.7487\n",
      "  Recall: 0.8934\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.70      0.78      2356\n",
      "           1       0.75      0.89      0.81      2354\n",
      "\n",
      "    accuracy                           0.80      4710\n",
      "   macro avg       0.81      0.80      0.79      4710\n",
      "weighted avg       0.81      0.80      0.79      4710\n",
      "\n",
      "\n",
      "Example Predictions (first 10):\n",
      "\n",
      "Example 1:\n",
      "  Query: Looking for a 5-door all-terrain electric vehicle with automatic transmission, black metallic color,...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [3.5967646e-04 3.6983230e-04 9.9927050e-01]\n",
      "  Weighted Score: 0.5996\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 2:\n",
      "  Query: Searching for an electric vehicle with a minimum of 6 airbags, LED headlights, and a park assistant....\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [1.9062022e-04 7.0642750e-04 9.9910307e-01]\n",
      "  Weighted Score: 0.6000\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 3:\n",
      "  Query: In search of an electric vehicle with automatic tailgate and lane keeping system. The car should hav...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [5.380698e-04 3.628933e-04 9.990990e-01]\n",
      "  Weighted Score: 0.5993\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 4:\n",
      "  Query: Interested in a 5-door electric vehicle with ABS and stability program. The car should have a power ...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [0.07935119 0.00242961 0.91821915]\n",
      "  Weighted Score: 0.4740\n",
      "  Binary Prediction: 0\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 5:\n",
      "  Query: Looking for an electric vehicle with cruise control and reverse camera. The car should have an autom...\n",
      "  True Label: 1\n",
      "  Probabilities [contradiction, neutral, entailment]: [1.6416598e-04 3.1508011e-04 9.9952078e-01]\n",
      "  Weighted Score: 0.5999\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 6:\n",
      "  Query: Searching for a petrol vehicle with a power output of less than 90 KW and mileage not exceeding 15,0...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [7.3820516e-03 7.1943032e-05 9.9254602e-01]\n",
      "  Weighted Score: 0.5882\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 7:\n",
      "  Query: In search of a diesel vehicle with automatic tailgate and lane keeping system. The car should have a...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [9.9934167e-01 7.1390030e-05 5.8700348e-04]\n",
      "  Weighted Score: -0.9989\n",
      "  Binary Prediction: 0\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 8:\n",
      "  Query: Interested in a 3-door electric vehicle with ABS and stability program. The car should have a power ...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [9.9516821e-01 3.9556634e-04 4.4363034e-03]\n",
      "  Weighted Score: -0.9921\n",
      "  Binary Prediction: 0\n",
      "  Correct: âœ“\n",
      "\n",
      "Example 9:\n",
      "  Query: Looking for a 5-door all-terrain petrol vehicle with automatic transmission. The car should have a p...\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [6.2656114e-03 2.1448609e-04 9.9351984e-01]\n",
      "  Weighted Score: 0.5901\n",
      "  Binary Prediction: 1\n",
      "  Correct: âœ—\n",
      "\n",
      "Example 10:\n",
      "  Query: Searching for an electric vehicle with a minimum of 8 airbags, LED headlights, and a park assistant....\n",
      "  True Label: 0\n",
      "  Probabilities [contradiction, neutral, entailment]: [9.9777681e-01 3.0009460e-04 1.9230366e-03]\n",
      "  Weighted Score: -0.9963\n",
      "  Binary Prediction: 0\n",
      "  Correct: âœ“\n"
     ]
    }
   ],
   "source": [
    "# Display results for DeBERTa\n",
    "results_deberta = evaluator_deberta.run_evaluation(train_vehicles_path, train_queries_path)\n",
    "evaluator_roberta.display_results(test_vehicles_path, test_queries_path, results_deberta, model_name='DeBERTa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sentence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
